ğŸš€ CIFAR-10 â€” Knowledge Distillation â†’ Pruning (Colab, Research-Friendly)




ğŸ¯ Goal: Apply Knowledge Distillation (KD) from ResNet-18 (Teacher) to MobileNetV3-Small (Student) and then compress with L1 unstructured pruning. Report the âš–ï¸ accuracy â€“ â±ï¸ latency â€“ ğŸ’¾ size trade-off in a reproducible, single-notebook workflow.

ğŸ” Summary

ğŸ—‚ï¸ Data: CIFAR-10 (inputs resized to 224Ã—224, normalized with ImageNet stats)

ğŸ§  Teacher: ResNet-18 (ImageNet-pretrained), briefly fine-tuned on CIFAR-10

ğŸ§© Student: MobileNetV3-Small (trained from scratch)

ğŸ§ª KD Loss: KL(T=4.0) + CE (Î±=0.9) + label smoothing=0.05

âœ‚ï¸ Pruning: L1 unstructured on Conv2d weights (suggested: 20â€“30%) + short fine-tune

ğŸ“ Metrics: Test Accuracy, single-sample GPU Latency (ms), parameter-based Model Size (MB)

ğŸ’¡ Note: Unstructured pruning doesnâ€™t shrink GPU kernel shapesâ€”speedup may be limited. For real speedups consider structured (channel/filter) pruning, INT8 (PTQ/QAT), or graph compilers (ONNX/TensorRT).

ğŸ—ƒï¸ Project Structure

cifar10-distill-prune-quant-colab/
 â”œâ”€ notebooks/
 â”‚   â””â”€ distill_prune_quant_colab.ipynb
 â”œâ”€ assets/
 â”‚   â”œâ”€ results_gpu_only.csv
 â”‚   â””â”€ metrics_grid.png
 â”œâ”€ README.md
 â”œâ”€ requirements.txt
 â”œâ”€ .gitignore
 â”œâ”€ LICENSE
 â””â”€ 
   ğŸ““ Notebook: notebooks/distill_prune_quant_colab.ipynb

   ğŸ“„ Results (CSV): assets/results_gpu_only.csv     


âš™ï¸ Local Setup & Run

python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
jupyter lab   # or jupyter notebook

â¡ï¸ In the notebook, follow: Teacher fine-tune â†’ KD â†’ Pruning + short fine-tune â†’ Evaluation & Plots.
ğŸ“¦ Outputs are saved under assets/: results_gpu_only.csv, metrics_grid.png.

ğŸ“ˆ Results

ğŸ–¼ï¸ Overview: assets/metrics_grid.png

ğŸ“„ Raw table: assets/results_gpu_only.csv

Example table (numbers vary by hardware/training budget):

| ğŸ“ Variant              | âœ… Accuracy | â±ï¸ LatencyMs | ğŸ’¾ SizeMB |
| ----------------------- | :--------: | -----------: | --------: |
| Teacher-ResNet18 (GPU)  |  0.80â€“0.92 |        \~2â€“4 |   \~44â€“46 |
| Student-KD-Pruned (GPU) |  0.60â€“0.75 |       \~5â€“10 |     \~5â€“7 |


ğŸ”¬ Method Details


âš™ï¸ Example hyperparams: LR=3e-4, weight_decay=1e-4, batch=128, Teacher epochsâ‰ˆ3, Student KD epochsâ‰ˆ6â€“8

ğŸ§ª Ablations to try:

KD temperature T âˆˆ {2,4,6}, Î± âˆˆ {0.5,0.7,0.9}

Pruning ratio p âˆˆ {0.2,0.3,0.5} (+ 1â€“2 epochs recovery)

Augmentations: RandAugment / Mixup

âš ï¸ Limitations: Unstructured pruning doesnâ€™t reduce kernel dimensions; GPU latency gains are limited.


ğŸ§­ Roadmap

ğŸ”§ Structured pruning (channel/filter) + retraining & real speed measurements

ğŸ§® INT8: PTQ/QAT; compare CPU vs GPU

ğŸš€ Compilation / acceleration via ONNX Runtime / TensorRT

ğŸŒ Generalization: CIFAR-100 or Tiny-ImageNet


âš–ï¸ License

This project is released under the MIT license (see LICENSE).













